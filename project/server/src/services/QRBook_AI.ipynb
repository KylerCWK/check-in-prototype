{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dCuEBT8qhVC"
      },
      "source": [
        "# Book Recommendation AI System\n",
        "\n",
        "This notebook implements a hybrid book recommendation system with two phases:\n",
        "1. Content-based filtering using BART summaries and metadata\n",
        "2. Neural Collaborative Filtering with content embeddings\n",
        "\n",
        "Key features:\n",
        "- Bias reduction and diversity promotion\n",
        "- Comprehensive evaluation metrics\n",
        "- Hybrid approach combining content and collaborative filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Er61sntmWvQK",
        "outputId": "558d24c3-6fda-4bbb-d4d4-cf4e2ac49059"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers sentence-transformers torch pandas numpy scikit-learn mongoengine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UEu_jGqNRRDT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEdFdRbgTyw"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5PzDTb0t7Vv"
      },
      "source": [
        "## Phase 1: Content-Based Filtering\n",
        "\n",
        "In this phase, we'll:\n",
        "1. Generate book summaries using BART\n",
        "2. Create embeddings using Sentence-BERT\n",
        "3. Implement content-based recommendations\n",
        "4. Add diversity-aware re-ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2qpIZgDegWD7"
      },
      "outputs": [],
      "source": [
        "class BARTSummarizer:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "        self.model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "        self.sentence_transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        inputs = self.tokenizer(text, max_length=1024, truncation=True, return_tensors='pt')\n",
        "        summary_ids = self.model.generate(inputs['input_ids'], num_beams=4, max_length=100,\n",
        "                                        early_stopping=True)\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def get_embedding(self, text):\n",
        "        return self.sentence_transformer.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZIIbnWWouJYR"
      },
      "outputs": [],
      "source": [
        "class ContentBasedRecommender:\n",
        "    def __init__(self, books_df):\n",
        "        self.books_df = books_df\n",
        "        self.summarizer = BARTSummarizer()\n",
        "        self.embeddings = None\n",
        "\n",
        "    def prepare_embeddings(self):\n",
        "        # Combine book metadata and summaries\n",
        "        combined_features = self.books_df.apply(\n",
        "            lambda x: f\"{x['title']} {x['author']} {x['genre']} {x['description']}\", axis=1\n",
        "        )\n",
        "        self.embeddings = np.vstack([\n",
        "            self.summarizer.get_embedding(text) for text in combined_features\n",
        "        ])\n",
        "\n",
        "    def get_recommendations(self, book_idx, n=5, diversity_weight=0.3):\n",
        "        # Calculate similarity scores\n",
        "        similarities = cosine_similarity([self.embeddings[book_idx]], self.embeddings)[0]\n",
        "\n",
        "        # Apply diversity re-ranking\n",
        "        recommendations = []\n",
        "        for _ in range(n):\n",
        "            if not recommendations:\n",
        "                idx = np.argmax(similarities)\n",
        "            else:\n",
        "                # Balance between similarity and diversity\n",
        "                diversity_scores = [1 - max([cosine_similarity(\n",
        "                    [self.embeddings[idx]], [self.embeddings[rec_idx]])[0][0]\n",
        "                    for rec_idx in recommendations]) for idx in range(len(similarities))]\n",
        "\n",
        "                combined_scores = (\n",
        "                    (1 - diversity_weight) * similarities +\n",
        "                    diversity_weight * np.array(diversity_scores)\n",
        "                )\n",
        "                idx = np.argmax(combined_scores)\n",
        "\n",
        "            recommendations.append(idx)\n",
        "            similarities[idx] = -1  # Exclude from future consideration\n",
        "\n",
        "        return recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv9U3zNau5vT"
      },
      "source": [
        "## Phase 2: Neural Collaborative Filtering\n",
        "\n",
        "Implementing a hybrid NCF model that incorporates:\n",
        "1. User-item interactions\n",
        "2. Content embeddings from Phase 1\n",
        "3. Fairness constraints and bias reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4fMMUmShu71U"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, embedding_dim, content_dim):\n",
        "        super(NCF, self).__init__()\n",
        "\n",
        "        # User and item embeddings\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # Content embedding projection\n",
        "        self.content_projection = nn.Linear(content_dim, embedding_dim)\n",
        "\n",
        "        # Layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(embedding_dim * 3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_idx, item_idx, content_embedding):\n",
        "        # Get embeddings\n",
        "        user_embed = self.user_embedding(user_idx)\n",
        "        item_embed = self.item_embedding(item_idx)\n",
        "        content_embed = self.content_projection(content_embedding)\n",
        "\n",
        "        # Combine embeddings\n",
        "        x = torch.cat([user_embed, item_embed, content_embed], dim=1)\n",
        "        return self.fc_layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TxTAHZ9gu-AN"
      },
      "outputs": [],
      "source": [
        "def train_ncf(model, train_loader, val_loader, num_epochs=10):\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for user, item, content, label in train_loader:\n",
        "            pred = model(user, item, content)\n",
        "            loss = criterion(pred, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for user, item, content, label in val_loader:\n",
        "                pred = model(user, item, content)\n",
        "                val_loss += criterion(pred, label).item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
        "        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n",
        "\n",
        "def evaluate_recommendations(predictions, true_labels, user_demographics=None):\n",
        "    metrics = {\n",
        "        'precision': precision_score(true_labels, predictions > 0.5),\n",
        "        'recall': recall_score(true_labels, predictions > 0.5),\n",
        "        'rmse': np.sqrt(mean_squared_error(true_labels, predictions)),\n",
        "        'mae': mean_absolute_error(true_labels, predictions)\n",
        "    }\n",
        "\n",
        "    if user_demographics is not None:\n",
        "        # Calculate demographic parity\n",
        "        predictions_by_group = {}\n",
        "        for demo_group in user_demographics.unique():\n",
        "            group_mask = user_demographics == demo_group\n",
        "            predictions_by_group[demo_group] = (predictions[group_mask] > 0.5).mean()\n",
        "\n",
        "        # Calculate max difference between groups\n",
        "        max_disparity = max(predictions_by_group.values()) - min(predictions_by_group.values())\n",
        "        metrics['demographic_parity'] = 1 - max_disparity\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V6owZu4Mu_0m"
      },
      "outputs": [],
      "source": [
        "def calculate_diversity_metrics(recommendations, book_embeddings):\n",
        "    if len(recommendations) < 2:\n",
        "        return {\n",
        "            'diversity_score': 0,\n",
        "            'coverage': 0\n",
        "        }\n",
        "\n",
        "    # Calculate pairwise distances between recommendations\n",
        "    pair_distances = []\n",
        "    for i in range(len(recommendations)):\n",
        "        for j in range(i + 1, len(recommendations)):\n",
        "            distance = 1 - cosine_similarity(\n",
        "                [book_embeddings[recommendations[i]]],\n",
        "                [book_embeddings[recommendations[j]]]\n",
        "            )[0][0]\n",
        "            pair_distances.append(distance)\n",
        "\n",
        "    # Calculate metrics\n",
        "    diversity_score = np.mean(pair_distances)\n",
        "    coverage = len(set(recommendations)) / len(book_embeddings)\n",
        "\n",
        "    return {\n",
        "        'diversity_score': diversity_score,\n",
        "        'coverage': coverage\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuziANvGw0UM"
      },
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "Load book data and user interactions from the database, then prepare it for both recommendation approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YXgSdD-_NcC",
        "outputId": "f1c5778d-9c59-4db5-9409-4bd6d25e0ba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MONGO_URI environment variable set from Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Assuming you've named your secret 'MONGO_URI' in the Colab Secrets manager\n",
        "mongo_uri = userdata.get('MONGO_URI')\n",
        "\n",
        "if mongo_uri:\n",
        "    os.environ['MONGO_URI'] = mongo_uri\n",
        "    print(\"MONGO_URI environment variable set from Colab Secrets.\")\n",
        "else:\n",
        "    print(\"Warning: 'MONGO_URI' secret not found in Colab Secrets.\")\n",
        "    # Handle this case - maybe prompt the user or exit if the connection is essential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "j2kSLRY0w0wx"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"Load book data and user interactions from MongoDB\"\"\"\n",
        "    import sys\n",
        "    import os\n",
        "    import inspect\n",
        "\n",
        "    current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "    path_to_add = None\n",
        "    max_levels_up = 5 # Check up to 5 levels up\n",
        "\n",
        "    for i in range(max_levels_up):\n",
        "        candidate_path = os.path.abspath(os.path.join(current_dir, *(['..'] * i)))\n",
        "        models_path_candidate = os.path.join(candidate_path, 'models')\n",
        "        if os.path.isdir(models_path_candidate):\n",
        "            path_to_add = candidate_path\n",
        "            print(f\"Found 'models' directory at: {models_path_candidate}\")\n",
        "            break # Found the path, exit loop\n",
        "\n",
        "    if path_to_add and path_to_add not in sys.path:\n",
        "        sys.path.insert(0, path_to_add)\n",
        "        print(f\"Added '{path_to_add}' to sys.path\")\n",
        "    elif not path_to_add:\n",
        "        print(f\"Error: Could not automatically find the directory containing 'models' within {max_levels_up} levels up.\")\n",
        "        print(\"Please manually specify the correct 'path_to_add'.\")\n",
        "        # Optionally, raise the error here if finding models is critical\n",
        "        # raise FileNotFoundError(\"Could not locate the 'models' directory.\")\n",
        "\n",
        "    # Import models - this will now look in the added parent directory\n",
        "    try:\n",
        "      from models.Book import Book\n",
        "      from models.User import User\n",
        "      from models.ReadingProfile import ReadingProfile\n",
        "      from config.db import connect_db # Assuming config is also in the parent dir\n",
        "      print(\"Successfully imported models and config.\")\n",
        "    except ModuleNotFoundError  as e:\n",
        "      print(f\"Failed to import models or config: {e}\")\n",
        "      print(\"Please ensure the correct path containing the 'models' and 'config' directories is added to sys.path.\")\n",
        "      print(f\"Current sys.path: {sys.path}\")\n",
        "      raise\n",
        "    # Connect to database\n",
        "    connect_db()\n",
        "\n",
        "    try:\n",
        "        # Assuming Book.objects.to_json() returns a valid JSON string\n",
        "        # that can be read by pd.read_json\n",
        "        # Add read_json parameters to handle potential objectid or other types if needed\n",
        "        # Example: `orient='records'` if to_json gives a list of dicts\n",
        "        # If to_json produces a string that is not a valid JSON array,\n",
        "        # you might need json.loads first.\n",
        "        books = pd.read_json(Book.objects.to_json())\n",
        "    except ValueError:\n",
        "        # If to_json returns a string that can't be directly read as JSON array\n",
        "        # Or if it's just a representation, you might need to process it\n",
        "        # Alternatively, if mongoengine QuerySet can be iterated:\n",
        "        print(\"Attempting to load books using list(Book.objects.values())\")\n",
        "        try:\n",
        "             # This assumes .values() returns a list of dictionaries suitable for DataFrame\n",
        "             books = pd.DataFrame(list(Book.objects.values()))\n",
        "        except Exception as e_values:\n",
        "             print(f\"Error loading books using .values(): {e_values}\")\n",
        "             print(\"Could not load book data.\")\n",
        "             # You might need more specific error handling or structure checking here\n",
        "             # Depending on the exact output of Book.objects\n",
        "             raise # Re-raise if book loading is essential\n",
        "\n",
        "    books['description'] = books['description'].fillna('')\n",
        "\n",
        "    # Load user interactions\n",
        "    interactions = []\n",
        "    # Iterate over ReadingProfile objects\n",
        "    try:\n",
        "        for profile in ReadingProfile.objects:\n",
        "            # Ensure user is an ObjectId and handle potential None\n",
        "            if profile.user:\n",
        "                user_id = str(profile.user.id) # Convert ObjectId to string\n",
        "            else:\n",
        "                continue # Skip profiles with no user\n",
        "\n",
        "            # Add positive interactions from favorites\n",
        "            if hasattr(profile, 'favorites') and profile.favorites:\n",
        "                 for book in profile.favorites:\n",
        "                      # Ensure book is an ObjectId and handle potential None\n",
        "                     if book:\n",
        "                         interactions.append({\n",
        "                             'user_id': user_id,\n",
        "                             'book_id': str(book.id), # Convert ObjectId to string\n",
        "                             'rating': 1\n",
        "                         })\n",
        "            # Add implicit negative interactions from viewed but not favorited\n",
        "            if hasattr(profile, 'viewed_books') and profile.viewed_books:\n",
        "                 for book in profile.viewed_books:\n",
        "                      # Ensure book is an ObjectId and handle potential None\n",
        "                     # Check if book.id (ObjectId) is in the list of favorite book ObjectIds\n",
        "                     favorite_ids = [fav_book.id for fav_book in (profile.favorites if hasattr(profile, 'favorites') else []) if fav_book]\n",
        "                     if book and book.id not in favorite_ids:\n",
        "                          interactions.append({\n",
        "                             'user_id': user_id,\n",
        "                             'book_id': str(book.id), # Convert ObjectId to string\n",
        "                             'rating': 0\n",
        "                         })\n",
        "    except Exception as e_interactions:\n",
        "         print(f\"Error loading interactions from ReadingProfile: {e_interactions}\")\n",
        "         # Depending on your application, you might continue or raise an error\n",
        "         # if interaction data is critical.\n",
        "         # For now, we'll proceed, but the interactions_df might be empty.\n",
        "\n",
        "\n",
        "    interactions_df = pd.DataFrame(interactions)\n",
        "\n",
        "    # Ensure book_id and user_id are consistent object types (strings)\n",
        "    if not interactions_df.empty:\n",
        "        interactions_df['book_id'] = interactions_df['book_id'].astype(str)\n",
        "        interactions_df['user_id'] = interactions_df['user_id'].astype(str)\n",
        "\n",
        "    # Ensure books_df index (which becomes book_id after map) is string if needed\n",
        "    # if not books_df.empty:\n",
        "    #    books_df.index = books_df.index.astype(str) # Might be needed depending on how book_map is created\n",
        "\n",
        "    return books, interactions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k2NWFNovw4Q3"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(books_df, interactions_df):\n",
        "    \"\"\"Prepare data for both recommendation approaches\"\"\"\n",
        "    # Create user and item mappings\n",
        "    user_map = {id_: idx for idx, id_ in enumerate(interactions_df['user_id'].unique())}\n",
        "    book_map = {id_: idx for idx, id_ in enumerate(books_df.index)}\n",
        "\n",
        "    # Transform interactions using mappings\n",
        "    interactions_df['user_idx'] = interactions_df['user_id'].map(user_map)\n",
        "    interactions_df['book_idx'] = interactions_df['book_id'].map(book_map)\n",
        "\n",
        "    # Split data\n",
        "    train_data, val_data = train_test_split(\n",
        "        interactions_df, test_size=0.2, random_state=42,\n",
        "        stratify=interactions_df['rating']\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'user_map': user_map,\n",
        "        'book_map': book_map,\n",
        "        'train_data': train_data,\n",
        "        'val_data': val_data,\n",
        "        'n_users': len(user_map),\n",
        "        'n_items': len(book_map)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH5cLxJYw7nt"
      },
      "source": [
        "## Public Datasets Integration\n",
        "\n",
        "Enrich our recommendation system with public datasets:\n",
        "1. Goodreads Dataset (Kaggle): 10M+ book ratings and metadata\n",
        "2. Amazon Customer Reviews: Book reviews and ratings\n",
        "3. Project Gutenberg: Text content for style analysis\n",
        "4. Open Library API: Rich book metadata\n",
        "\n",
        "This helps address the cold-start problem and improves recommendation quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LV0-Yep8w77c"
      },
      "outputs": [],
      "source": [
        "def load_goodreads_data(data_path='../data/goodreads'):\n",
        "    \"\"\"Load and preprocess Goodreads dataset\n",
        "    Download from: https://www.kaggle.com/datasets/bahramjannesarr/goodreads-book-datasets-10m\n",
        "    \"\"\"\n",
        "    # Load ratings and books\n",
        "    ratings = pd.read_csv(f'{data_path}/ratings.csv')\n",
        "    books = pd.read_csv(f'{data_path}/books.csv')\n",
        "\n",
        "    # Clean and preprocess\n",
        "    books['description'] = books['description'].fillna('')\n",
        "    books['genres'] = books['genres'].fillna('')\n",
        "\n",
        "    # Convert ratings to binary interactions (rating >= 4 considered positive)\n",
        "    ratings['rating'] = (ratings['rating'] >= 4).astype(int)\n",
        "\n",
        "    # Sample a manageable subset for training\n",
        "    n_users = 10000\n",
        "    n_books = 5000\n",
        "\n",
        "    # Get top users and books by interaction count\n",
        "    top_users = ratings['user_id'].value_counts().nlargest(n_users).index\n",
        "    top_books = ratings['book_id'].value_counts().nlargest(n_books).index\n",
        "\n",
        "    # Filter ratings\n",
        "    ratings_subset = ratings[\n",
        "        ratings['user_id'].isin(top_users) &\n",
        "        ratings['book_id'].isin(top_books)\n",
        "    ]\n",
        "\n",
        "    # Get corresponding books\n",
        "    books_subset = books[books['book_id'].isin(top_books)]\n",
        "\n",
        "    return books_subset, ratings_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7WY99w7aw-xw"
      },
      "outputs": [],
      "source": [
        "def load_amazon_reviews(data_path='../data/amazon'):\n",
        "    \"\"\"Load and preprocess Amazon book reviews dataset\n",
        "    Download from: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz\n",
        "    \"\"\"\n",
        "    # Load reviews\n",
        "    reviews = pd.read_csv(\n",
        "        f'{data_path}/amazon_reviews_us_Books_v1_02.tsv',\n",
        "        sep='\\t',\n",
        "        usecols=[\n",
        "            'customer_id', 'product_id', 'product_title',\n",
        "            'star_rating', 'review_body'\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Clean and preprocess\n",
        "    reviews = reviews.dropna(subset=['review_body'])\n",
        "\n",
        "    # Convert ratings to binary interactions (rating >= 4 considered positive)\n",
        "    reviews['rating'] = (reviews['star_rating'] >= 4).astype(int)\n",
        "\n",
        "    # Create books dataframe\n",
        "    books = reviews[['product_id', 'product_title']].drop_duplicates()\n",
        "    books.columns = ['book_id', 'title']\n",
        "\n",
        "    # Sample a manageable subset\n",
        "    n_users = 10000\n",
        "    n_books = 5000\n",
        "\n",
        "    # Get top users and books\n",
        "    top_users = reviews['customer_id'].value_counts().nlargest(n_users).index\n",
        "    top_books = reviews['product_id'].value_counts().nlargest(n_books).index\n",
        "\n",
        "    # Filter reviews\n",
        "    reviews_subset = reviews[\n",
        "        reviews['customer_id'].isin(top_users) &\n",
        "        reviews['product_id'].isin(top_books)\n",
        "    ][['customer_id', 'product_id', 'rating', 'review_body']]\n",
        "\n",
        "    # Get corresponding books\n",
        "    books_subset = books[books['book_id'].isin(top_books)]\n",
        "\n",
        "    return books_subset, reviews_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d8abZdftw__c"
      },
      "outputs": [],
      "source": [
        "def enrich_with_open_library(books_df):\n",
        "    \"\"\"Enrich book metadata using Open Library API\"\"\"\n",
        "    import requests\n",
        "    import time\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    def fetch_book_data(title, author=None):\n",
        "        # Search Open Library\n",
        "        query = f'title:{title}'\n",
        "        if author:\n",
        "            query += f' author:{author}'\n",
        "\n",
        "        response = requests.get(\n",
        "            'https://openlibrary.org/search.json',\n",
        "            params={'q': query}\n",
        "        )\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data['docs']:\n",
        "                book = data['docs'][0]\n",
        "                return {\n",
        "                    'ol_id': book.get('key', ''),\n",
        "                    'subjects': book.get('subject', []),\n",
        "                    'first_publish_year': book.get('first_publish_year', None),\n",
        "                    'language': book.get('language', []),\n",
        "                    'ebook_available': book.get('ebook_access', '') != 'no_ebook'\n",
        "                }\n",
        "        return None\n",
        "\n",
        "    # Enrich each book\n",
        "    enriched_data = []\n",
        "    for _, book in tqdm(books_df.iterrows(), total=len(books_df)):\n",
        "        data = fetch_book_data(book['title'], book.get('author'))\n",
        "        if data:\n",
        "            enriched_data.append(data)\n",
        "        else:\n",
        "            enriched_data.append({\n",
        "                'ol_id': '',\n",
        "                'subjects': [],\n",
        "                'first_publish_year': None,\n",
        "                'language': [],\n",
        "                'ebook_available': False\n",
        "            })\n",
        "        time.sleep(1)  # Rate limiting\n",
        "\n",
        "    # Add enriched data to dataframe\n",
        "    enriched_df = pd.DataFrame(enriched_data)\n",
        "    return pd.concat([books_df, enriched_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "APOCSd-rxBmk"
      },
      "outputs": [],
      "source": [
        "def combine_data_sources(use_public_data=True):\n",
        "    \"\"\"Combine internal database with public datasets\"\"\"\n",
        "    # Load internal data\n",
        "    internal_books, internal_interactions = load_data()\n",
        "\n",
        "    if not use_public_data:\n",
        "        return internal_books, internal_interactions\n",
        "\n",
        "    # Load public datasets\n",
        "    try:\n",
        "        goodreads_books, goodreads_ratings = load_goodreads_data()\n",
        "        amazon_books, amazon_reviews = load_amazon_reviews()\n",
        "\n",
        "        # Combine books\n",
        "        all_books = pd.concat([\n",
        "            internal_books,\n",
        "            goodreads_books.add_prefix('gr_'),\n",
        "            amazon_books.add_prefix('amzn_')\n",
        "        ], axis=0)\n",
        "\n",
        "        # Combine interactions\n",
        "        all_interactions = pd.concat([\n",
        "            internal_interactions,\n",
        "            goodreads_ratings.add_prefix('gr_'),\n",
        "            amazon_reviews.add_prefix('amzn_')\n",
        "        ], axis=0)\n",
        "\n",
        "        # Enrich with Open Library data\n",
        "        print(\"Enriching with Open Library data...\")\n",
        "        all_books = enrich_with_open_library(all_books)\n",
        "\n",
        "        return all_books, all_interactions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load public datasets: {str(e)}\")\n",
        "        print(\"Falling back to internal data only.\")\n",
        "        return internal_books, internal_interactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf-mMyY6xEIi"
      },
      "source": [
        "## Main Execution\n",
        "\n",
        "Put everything together and train both recommendation systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX2yAsgP33Ac",
        "outputId": "ad945064-6427-4def-8c79-28c01afa64ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mongoengine in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: pymongo<5.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from mongoengine) (4.13.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pymongo<5.0,>=3.4->mongoengine) (2.7.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade mongoengine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "collapsed": true,
        "id": "fAt7wPF-xFFn",
        "outputId": "6ecd5021-3ddf-4d74-f9b6-ff9516efa425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 'models' directory at: /content/models\n",
            "Successfully imported models and config.\n",
            "Error connecting to MongoDB: A different connection with alias `default` was already registered. Use disconnect() first\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-37f4148a46d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage of combined datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbooks_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractions_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_data_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_public_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total books: {len(books_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total interactions: {len(interactions_df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5720bce83972>\u001b[0m in \u001b[0;36mcombine_data_sources\u001b[0;34m(use_public_data)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Combine internal database with public datasets\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load internal data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minternal_books\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_interactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_public_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-f2b71015630d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# If to_json produces a string that is not a valid JSON array,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# you might need json.loads first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mbooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# If to_json returns a string that can't be directly read as JSON array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mongoengine/queryset/base.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1327\u001b[0m             )\n\u001b[1;32m   1328\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"json_options\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLEGACY_JSON_OPTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_pymongo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bson/json_util.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \"\"\"\n\u001b[1;32m    472\u001b[0m     \u001b[0mjson_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_JSON_OPTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_json_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bson/json_util.py\u001b[0m in \u001b[0;36m_json_convert\u001b[0;34m(obj, json_options)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_json_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_json_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/bson/json_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_json_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_json_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mongoengine/queryset/queryset.py\u001b[0m in \u001b[0;36m_iter_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# Otherwise, populate more of the cache and repeat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_populate_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mongoengine/queryset/queryset.py\u001b[0m in \u001b[0;36m_populate_cache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITER_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Getting this exception means there are no more docs in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mongoengine/queryset/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m         \u001b[0mraw_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_pymongo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_DocumentType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCursor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_DocumentType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exhaust\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             )\n\u001b[0;32m-> 1208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Get More\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/cursor.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             response = client._run_operation(\n\u001b[0m\u001b[1;32m   1103\u001b[0m                 \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unpack_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/_csot.py\u001b[0m in \u001b[0;36mcsot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0m_TimeoutContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsot_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_run_operation\u001b[0;34m(self, operation, unpack_res, address)\u001b[0m\n\u001b[1;32m   1915\u001b[0m             )\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         return self._retryable_read(\n\u001b[0m\u001b[1;32m   1918\u001b[0m             \u001b[0m_cmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_retryable_read\u001b[0;34m(self, func, read_pref, session, operation, address, retryable, operation_id)\u001b[0m\n\u001b[1;32m   2024\u001b[0m             \u001b[0mretryable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_reads\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_transaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m         )\n\u001b[0;32m-> 2026\u001b[0;31m         return self._retry_internal(\n\u001b[0m\u001b[1;32m   2027\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2028\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/_csot.py\u001b[0m in \u001b[0;36mcsot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0m_TimeoutContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsot_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_retry_internal\u001b[0;34m(self, func, session, bulk, operation, is_read, address, read_pref, retryable, operation_id)\u001b[0m\n\u001b[1;32m   1991\u001b[0m             \u001b[0mretryable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretryable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0moperation_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperation_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m         ).run()\n\u001b[0m\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m     def _retryable_read(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_last_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_csot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2730\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_read\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mServerSelectionTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2732\u001b[0m                 \u001b[0;31m# The application may think the write was never attempted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2873\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m         \"\"\"\n\u001b[0;32m-> 2875\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2876\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_pref\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read Preference required on read calls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m         with self._client._conn_from_server(self._read_pref, self._server, self._session) as (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_get_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2821\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAbstraction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mconnect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m         \"\"\"\n\u001b[0;32m-> 2823\u001b[0;31m         return self._client._select_server(\n\u001b[0m\u001b[1;32m   2824\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_selector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/mongo_client.py\u001b[0m in \u001b[0;36m_select_server\u001b[0;34m(self, server_selector, session, operation, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mAutoReconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server %s:%s no longer available\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: UP031\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1812\u001b[0;31m                 server = topology.select_server(\n\u001b[0m\u001b[1;32m   1813\u001b[0m                     \u001b[0mserver_selector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m                     \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/topology.py\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    407\u001b[0m     ) -> Server:\n\u001b[1;32m    408\u001b[0m         \u001b[0;34m\"\"\"Like select_servers, but choose a random server if several match.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         server = self._select_server(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/topology.py\u001b[0m in \u001b[0;36m_select_server\u001b[0;34m(self, selector, operation, server_selection_timeout, address, deprioritized_servers, operation_id)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0moperation_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     ) -> Server:\n\u001b[0;32m--> 387\u001b[0;31m         servers = self.select_servers(\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_selection_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/topology.py\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, operation, server_selection_timeout, address, operation_id)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             server_descriptions = self._select_servers_loop(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/synchronous/topology.py\u001b[0m in \u001b[0;36m_select_servers_loop\u001b[0;34m(self, selector, timeout, operation, operation_id, address)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# came after our most recent apply_selector call, since we've\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# held the lock until now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0m_cond_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_HEARTBEAT_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0mnow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pymongo/lock.py\u001b[0m in \u001b[0;36m_cond_wait\u001b[0;34m(condition, timeout)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cond_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example usage of combined datasets\n",
        "books_df, interactions_df = combine_data_sources(use_public_data=True)\n",
        "print(f\"Total books: {len(books_df)}\")\n",
        "print(f\"Total interactions: {len(interactions_df)}\")\n",
        "\n",
        "# Display sample of enriched book data\n",
        "print(\"\\nSample of enriched book data:\")\n",
        "print(books_df[['title', 'author', 'genre', 'ol_id', 'subjects', 'first_publish_year']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLoZRNKpxHZn"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading data...\")\n",
        "    books_df, interactions_df = load_data()\n",
        "    data = preprocess_data(books_df, interactions_df)\n",
        "\n",
        "    # Initialize and train content-based system\n",
        "    print(\"\\nTraining content-based system...\")\n",
        "    content_recommender = ContentBasedRecommender(books_df)\n",
        "    content_recommender.prepare_embeddings()\n",
        "\n",
        "    # Initialize and train NCF model\n",
        "    print(\"\\nTraining neural collaborative filtering model...\")\n",
        "    EMBEDDING_DIM = 64\n",
        "    CONTENT_DIM = 384  # dimension of Sentence-BERT embeddings\n",
        "\n",
        "    model = NCF(\n",
        "        num_users=data['n_users'],\n",
        "        num_items=data['n_items'],\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        content_dim=CONTENT_DIM\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "    def create_loader(df, batch_size=32):\n",
        "        users = torch.LongTensor(df['user_idx'].values)\n",
        "        items = torch.LongTensor(df['book_idx'].values)\n",
        "        # Get content embeddings for each book\n",
        "        contents = torch.FloatTensor(np.array([\n",
        "            content_recommender.embeddings[idx] for idx in df['book_idx'].values\n",
        "        ]))\n",
        "        labels = torch.FloatTensor(df['rating'].values)\n",
        "        return DataLoader(\n",
        "            TensorDataset(users, items, contents, labels),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "    train_loader = create_loader(data['train_data'])\n",
        "    val_loader = create_loader(data['val_data'])\n",
        "\n",
        "    # Train the model\n",
        "    train_ncf(model, train_loader, val_loader, num_epochs=10)\n",
        "\n",
        "    print(\"\\nTraining complete! Models are ready for making recommendations.\")\n",
        "\n",
        "    # Save the trained models\n",
        "    import joblib\n",
        "    import torch\n",
        "\n",
        "    # Save content-based recommender\n",
        "    joblib.dump(content_recommender, '../models/content_recommender.pkl')\n",
        "\n",
        "    # Save NCF model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'user_map': data['user_map'],\n",
        "        'book_map': data['book_map']\n",
        "    }, '../models/ncf_model.pth')\n",
        "\n",
        "    print(\"\\nModels saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHBSx5PG5AfT",
        "outputId": "3d33cf2f-f7b2-4aee-c80e-efe1bdd9f5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory 'models' created.\n",
            "Directory 'config' created.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the path for the directories you want to create\n",
        "# For example, to create a 'models' directory and a 'config' directory\n",
        "models_dir = 'models'\n",
        "config_dir = 'config'\n",
        "\n",
        "# Create the 'models' directory\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)\n",
        "    print(f\"Directory '{models_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{models_dir}' already exists.\")\n",
        "\n",
        "# Create the 'config' directory\n",
        "if not os.path.exists(config_dir):\n",
        "    os.makedirs(config_dir)\n",
        "    print(f\"Directory '{config_dir}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{config_dir}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPw26o3m5SA7",
        "outputId": "e164c07f-bc27-4ee1-f6bd-ebba3c453a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "config\tmodels\tsample_data\n"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
